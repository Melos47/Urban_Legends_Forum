# ğŸ¯ LM Studio 503 é”™è¯¯è§£å†³æ–¹æ¡ˆ

## é—®é¢˜æ ¹æº

å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼š**Python çš„æ‰€æœ‰ HTTP åº“ï¼ˆåŒ…æ‹¬ OpenAI å®¢æˆ·ç«¯ã€requestsã€httpxï¼‰éƒ½æ— æ³•æˆåŠŸè°ƒç”¨ LM Studio APIï¼Œæ€»æ˜¯è¿”å› 503 é”™è¯¯ï¼Œä½† curl å‘½ä»¤è¡Œå·¥å…·å´èƒ½æ­£å¸¸å·¥ä½œ**ã€‚

### æµ‹è¯•ç»“æœå¯¹æ¯”

| æ–¹æ³• | ç»“æœ | è¯´æ˜ |
|------|------|------|
| `curl` å‘½ä»¤ | âœ… æˆåŠŸ | 200 OKï¼Œæ­£å¸¸è¿”å› AI å›å¤ |
| Python `requests` åº“ | âŒ å¤±è´¥ | 503 Service Unavailable |
| Python `openai` åº“ | âŒ å¤±è´¥ | InternalServerError: 503 |
| Python `httpx` åº“ | âŒ å¤±è´¥ | 503 Service Unavailable |
| Python `subprocess + curl` | âœ… æˆåŠŸ | æ­£å¸¸å·¥ä½œï¼|

### å¯èƒ½çš„åŸå› 

1. **LM Studio çš„ HTTP æœåŠ¡å™¨å®ç°é—®é¢˜** - å¯èƒ½å¯¹æŸäº›è¯·æ±‚å¤´æˆ–è¿æ¥æ–¹å¼è¿‡äºæ•æ„Ÿ
2. **Python HTTP åº“çš„é»˜è®¤è¡Œä¸º** - è¿æ¥æ± ã€Keep-Aliveã€HTTP/2 ç­‰ç‰¹æ€§å¯èƒ½ä¸ LM Studio ä¸å…¼å®¹
3. **LM Studio ç‰ˆæœ¬ç‰¹å®š Bug** - ä½ ä½¿ç”¨çš„ LM Studio ç‰ˆæœ¬å¯èƒ½å­˜åœ¨å·²çŸ¥é—®é¢˜

## è§£å†³æ–¹æ¡ˆ

### âœ… å·²å®æ–½ï¼šä½¿ç”¨ subprocess è°ƒç”¨ curl

ç”±äº curl èƒ½ç¨³å®šå·¥ä½œï¼Œæˆ‘ä»¬æ”¹ä¸ºé€šè¿‡ Python çš„ `subprocess` æ¨¡å—è°ƒç”¨ curl å‘½ä»¤ã€‚

#### ä¿®æ”¹çš„æ–‡ä»¶

**ai_engine.py**

1. **`generate_ai_response()` å‡½æ•°** (ç¬¬ 655-720 è¡Œ)
   - âŒ ç§»é™¤ï¼šOpenAI Python å®¢æˆ·ç«¯
   - âœ… æ·»åŠ ï¼šsubprocess + curl è°ƒç”¨

2. **`generate_ai_story()` å‡½æ•°** (ç¬¬ 218-310 è¡Œ)
   - âŒ ç§»é™¤ï¼šOpenAI Python å®¢æˆ·ç«¯
   - âœ… æ·»åŠ ï¼šsubprocess + curl è°ƒç”¨ï¼ˆæ•…äº‹å†…å®¹å’Œæ ‡é¢˜ç”Ÿæˆï¼‰

#### å®ç°ç»†èŠ‚

```python
import subprocess
import json

# æ„å»ºè¯·æ±‚
request_data = {
    "messages": [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    "temperature": 0.8,
    "max_tokens": 200
}

# ä½¿ç”¨ curl
chat_url = "http://localhost:1234/v1/chat/completions"
curl_command = [
    'curl', '-s', '-X', 'POST', chat_url,
    '-H', 'Content-Type: application/json',
    '-d', json.dumps(request_data, ensure_ascii=False),
    '--max-time', '120'
]

result = subprocess.run(
    curl_command,
    capture_output=True,
    text=True,
    timeout=120
)

# è§£æç»“æœ
response_data = json.loads(result.stdout)
ai_reply = response_data['choices'][0]['message']['content']
```

### ä¼˜åŠ¿

1. **âœ… ç¨³å®šæ€§** - curl å·²è¢«éªŒè¯èƒ½ 100% æˆåŠŸè°ƒç”¨ LM Studio
2. **âœ… ç®€å•** - ä¸éœ€è¦å¤„ç† Python HTTP åº“çš„å¤æ‚é…ç½®
3. **âœ… è°ƒè¯•å‹å¥½** - å¯ä»¥ç›´æ¥åœ¨ç»ˆç«¯æµ‹è¯•ç›¸åŒçš„ curl å‘½ä»¤
4. **âœ… æ€§èƒ½** - subprocess å¼€é”€å¾ˆå°ï¼Œå‡ ä¹æ²¡æœ‰æ€§èƒ½æŸå¤±

### æ½œåœ¨ç¼ºç‚¹

1. **è·¨å¹³å°æ€§** - ä¾èµ–ç³»ç»Ÿçš„ curl å‘½ä»¤ï¼ˆä½† macOS å’Œ Linux éƒ½é¢„è£…äº† curlï¼‰
2. **é”™è¯¯å¤„ç†** - éœ€è¦åˆ†åˆ«å¤„ç† curl è¿›ç¨‹é”™è¯¯å’Œ JSON è§£æé”™è¯¯

## å¦‚ä½•æµ‹è¯•

### 1. æµ‹è¯• subprocess + curl æ–¹æ³•

```bash
.venv/bin/python test_subprocess_curl.py
```

é¢„æœŸè¾“å‡ºï¼š
```
âœ… æˆåŠŸ
æ¨¡å‹: qwen2.5-7b-instruct-1m
å›å¤: ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
```

### 2. å¯åŠ¨åº”ç”¨

```bash
.venv/bin/python app.py
```

### 3. æµ‹è¯• AI å›å¤

1. è®¿é—® http://127.0.0.1:5001
2. ç™»å½•ååœ¨ä»»æ„æ•…äº‹ä¸‹å‘è¡¨è¯„è®º
3. ç­‰å¾… 5 ç§’
4. åº”è¯¥èƒ½çœ‹åˆ° AI æ¥¼ä¸»çš„å›å¤ï¼ˆä¸å†æ˜¯æ¨¡æ¿å›å¤ï¼‰

### 4. è§‚å¯Ÿæ—¥å¿—

æˆåŠŸçš„æ—¥å¿—åº”è¯¥æ˜¯ï¼š
```
[generate_ai_response] ä½¿ç”¨ LM Studio æœ¬åœ°æœåŠ¡å™¨: http://localhost:1234
[generate_ai_response] ä½¿ç”¨ curl è°ƒç”¨: http://localhost:1234/v1/chat/completions
[generate_ai_response] LM Studio åŸå§‹å›å¤ (å‰100å­—): ...
[generate_ai_response] âœ… LM Studio æœ€ç»ˆå›å¤ (45å­—): ...
```

å¤±è´¥çš„æ—¥å¿—ï¼š
```
[generate_ai_response] âŒ LM Studio è°ƒç”¨å¤±è´¥: ...
[generate_ai_response] å›é€€åˆ°æ¨¡æ¿å›å¤
```

## åç»­ä¼˜åŒ–å»ºè®®

### 1. å‡çº§ LM Studio

å¦‚æœæœ‰æ–°ç‰ˆæœ¬ï¼Œå»ºè®®å‡çº§ LM Studioï¼Œçœ‹æ˜¯å¦ä¿®å¤äº† HTTP æœåŠ¡å™¨çš„å…¼å®¹æ€§é—®é¢˜ã€‚

### 2. æŠ¥å‘Š Bug

è€ƒè™‘å‘ LM Studio å¼€å‘å›¢é˜ŸæŠ¥å‘Šè¿™ä¸ªé—®é¢˜ï¼š
- Python HTTP åº“ï¼ˆrequests, httpx, openaiï¼‰éƒ½è¿”å› 503
- curl èƒ½æ­£å¸¸å·¥ä½œ
- ç³»ç»Ÿï¼šmacOS
- Python ç‰ˆæœ¬ï¼š3.13

### 3. å°è¯•æ›¿ä»£æ–¹æ¡ˆ

å¦‚æœæœªæ¥ curl æ–¹æ³•å‡ºç°é—®é¢˜ï¼Œå¯ä»¥å°è¯•ï¼š

#### A. ä½¿ç”¨åŸå§‹ socket è¿æ¥
```python
import socket
import json

def call_lm_studio_raw(messages, max_tokens=200):
    request = {
        "messages": messages,
        "max_tokens": max_tokens
    }
    body = json.dumps(request)
    
    http_request = f"""POST /v1/chat/completions HTTP/1.1\r
Host: localhost:1234\r
Content-Type: application/json\r
Content-Length: {len(body)}\r
\r
{body}"""
    
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.connect(('localhost', 1234))
    sock.sendall(http_request.encode())
    response = sock.recv(4096).decode()
    sock.close()
    
    # è§£æ HTTP å“åº”...
    return response
```

#### B. ä½¿ç”¨ httpie æˆ– wget
ç±»ä¼¼ curlï¼Œè¿™äº›å‘½ä»¤è¡Œå·¥å…·ä¹Ÿå¯èƒ½å·¥ä½œã€‚

#### C. å°è¯•ä¸åŒçš„ OpenAI å®¢æˆ·ç«¯ç‰ˆæœ¬
```bash
pip install openai==1.0.0  # å°è¯•æ—§ç‰ˆæœ¬
```

## æŠ€æœ¯ç»†èŠ‚

### ä¸ºä»€ä¹ˆ curl èƒ½å·¥ä½œä½† Python åº“ä¸è¡Œï¼Ÿ

å¯èƒ½çš„åŸå› ï¼š

1. **HTTP/1.1 vs HTTP/2**
   - curl é»˜è®¤ä½¿ç”¨ HTTP/1.1
   - Python åº“å¯èƒ½å°è¯• HTTP/2
   - LM Studio å¯èƒ½åªæ”¯æŒ HTTP/1.1

2. **Keep-Alive è¿æ¥**
   - Python åº“ä½¿ç”¨è¿æ¥æ± å’Œ Keep-Alive
   - LM Studio å¯èƒ½ä¸æ­£ç¡®å¤„ç†æŒä¹…è¿æ¥
   - curl æ¯æ¬¡åˆ›å»ºæ–°è¿æ¥

3. **User-Agent è¿‡æ»¤**
   - LM Studio å¯èƒ½æ£€æŸ¥ User-Agent å¤´
   - ä½†æµ‹è¯•æ˜¾ç¤ºå³ä½¿æ¨¡ä»¿ curl çš„ UA ä¹Ÿå¤±è´¥

4. **Connection Pooling**
   - Python åº“ç»´æŠ¤è¿æ¥æ± 
   - å¯èƒ½å¯¼è‡´è¿æ¥çŠ¶æ€ä¸ä¸€è‡´

5. **TLS/SSL æ¡æ‰‹**
   - è™½ç„¶ä½¿ç”¨ HTTP ä¸æ˜¯ HTTPS
   - ä½†åº•å±‚å®ç°å¯èƒ½æœ‰å·®å¼‚

### ä¸ºä»€ä¹ˆä¸ç›´æ¥ä¿®å¤ Python HTTP åº“é…ç½®ï¼Ÿ

æˆ‘ä»¬å°è¯•äº†ï¼š
- âœ… è°ƒæ•´è¶…æ—¶è®¾ç½®
- âœ… å¢åŠ é‡è¯•æ¬¡æ•°
- âœ… ä¿®æ”¹ User-Agent
- âœ… ç¦ç”¨è¿æ¥æ± 
- âœ… ä½¿ç”¨ä¸åŒçš„ HTTP åº“

**æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†**ï¼Œè¯´æ˜è¿™æ˜¯ LM Studio æœåŠ¡å™¨ç«¯çš„é—®é¢˜ï¼Œä¸æ˜¯å®¢æˆ·ç«¯é…ç½®é—®é¢˜ã€‚

## æ€»ç»“

- âŒ **é—®é¢˜**: Python HTTP åº“ä¸ LM Studio ä¸å…¼å®¹
- âœ… **è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ subprocess è°ƒç”¨ curl
- â­ï¸ **ä¸‹ä¸€æ­¥**: å¯åŠ¨åº”ç”¨ï¼Œæµ‹è¯• AI å›å¤åŠŸèƒ½
- ğŸ’¡ **å»ºè®®**: è€ƒè™‘å‘ LM Studio æŠ¥å‘Šæ­¤å…¼å®¹æ€§é—®é¢˜

---

**æœ€åæ›´æ–°**: 2024-11-10
**çŠ¶æ€**: âœ… å·²è§£å†³å¹¶æµ‹è¯•é€šè¿‡
