import os
import random
from datetime import datetime
from openai import OpenAI
from anthropic import Anthropic
import requests
from PIL import Image
from io import BytesIO

# Initialize AI clients
openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

# Horror story personas for AI
AI_PERSONAS = [
    {'name': 'æ·±å¤œç›®å‡»è€…', 'emoji': 'ğŸ‘ï¸', 'style': 'witness'},
    {'name': 'éƒ½å¸‚è°ƒæŸ¥å‘˜', 'emoji': 'ï¿½ï¿½', 'style': 'investigator'},
    {'name': 'åŒ¿åä¸¾æŠ¥äºº', 'emoji': 'ğŸ•µï¸', 'style': 'whistleblower'},
    {'name': 'å¤±è¸ªè€…æ—¥è®°', 'emoji': 'ğŸ“”', 'style': 'victim'},
    {'name': 'åœ°é“å®ˆå¤œäºº', 'emoji': 'ğŸš‡', 'style': 'worker'}
]

# Urban legend categories
LEGEND_CATEGORIES = [
    'subway_ghost',
    'abandoned_building',
    'cursed_object',
    'missing_person',
    'time_anomaly',
    'shadow_figure',
    'haunted_electronics'
]

# Locations in Hong Kong
CITY_LOCATIONS = [
    'æ—ºè§’é‡‘é±¼è¡—',
    'æ²¹éº»åœ°æˆé™¢',
    'ä¸­ç¯è‡³åŠå±±è‡ªåŠ¨æ‰¶æ¢¯',
    'å½©è™¹é‚¨',
    'æ€ªå…½å¤§å¦ (é²—é±¼æ¶Œ)',
    'é‡åº†å¤§å¦',
    'è¾¾å¾·å­¦æ ¡ (å…ƒæœ—å±å±±)',
    'è¥¿è´¡ç»“ç•Œ',
    'å¤§åŸ”é“è·¯åšç‰©é¦†',
    'é«˜è¡—é¬¼å±‹ (è¥¿è¥ç›˜ç¤¾åŒºç»¼åˆå¤§æ¥¼)'
]

def generate_story_prompt(category, location, persona):
    """Generate prompt for AI story creation"""
    prompts = {
        'subway_ghost': f"ä½œä¸º{persona['name']}ï¼Œè®²è¿°ä½ åœ¨æ¸¯é“{location}ç«™æ·±å¤œé­é‡çš„è¯¡å¼‚ç»å†ã€‚æè¿°å…·ä½“çš„æ—¶é—´ã€ç©ºæ— ä¸€äººçš„è½¦å¢ã€å¬åˆ°çš„æ€ªå£°æˆ–çœ‹åˆ°çš„å¼‚å¸¸å€’å½±ã€‚è¯­æ°”è¦çœŸå®ï¼Œåƒåœ¨è®ºå›ä¸Šåˆ†äº«äº²èº«ç»å†ã€‚",
        'abandoned_building': f"ä½ æ˜¯{persona['name']}ï¼Œæœ€è¿‘åœ¨{location}æ¢é™©æ—¶å‘ç°äº†ä»¤äººä¸å®‰çš„ç§˜å¯†ã€‚è¯¦ç»†æè¿°å»ºç­‘å†…éƒ¨çš„è’åºŸæ™¯è±¡ã€å‘ç°çš„æ—§ç‰©ä»¶ï¼ˆä¾‹å¦‚80å¹´ä»£çš„æŠ¥çº¸ã€å¥‡æ€ªçš„ç¬¦å’’ï¼‰ã€ä»¥åŠè®©ä½ æ¯›éª¨æ‚šç„¶çš„è¶…è‡ªç„¶ç°è±¡ã€‚",
        'cursed_object': f"ä½œä¸º{persona['name']}ï¼Œä½ åœ¨{location}é™„è¿‘çš„ä¸€ä¸ªå°æ‘Šä¸Šä¹°åˆ°äº†ä¸€ä¸ªè¢«è¯…å’’çš„ç‰©å“ï¼ˆå¦‚ä¸€ä¸ªæ—§ç½—ç›˜ã€ä¸€ä¸ªç‰ä½©ï¼‰ã€‚è®²è¿°ç‰©å“çš„æ¥å†ã€è·å¾—çš„è¿‡ç¨‹ã€ä»¥åŠä¹‹åå‘ç”Ÿçš„è¿ä¸²æ€ªäº‹ã€‚",
        'missing_person': f"ä½ æ˜¯{persona['name']}ï¼Œæ­£åœ¨è°ƒæŸ¥ä¸€å®—å‘ç”Ÿåœ¨{location}çš„ç¦»å¥‡å¤±è¸ªæ¡ˆã€‚æä¾›æ¡ˆä»¶ç»†èŠ‚ã€å¤±è¸ªè€…æœ€åçš„è¡Œè¸ªï¼ˆä¾‹å¦‚CCTVæœ€åæ‹åˆ°çš„ç”»é¢ï¼‰ã€ä»¥åŠä½ å‘ç°çš„æ— æ³•ç”¨å¸¸ç†è§£é‡Šçš„çº¿ç´¢ã€‚",
        'time_anomaly': f"ä½œä¸º{persona['name']}ï¼Œä½ åœ¨{location}çš„æŸæ¡åå··ç»å†äº†æ—¶é—´é”™ä½ã€‚æè¿°å‘¨å›´ç¯å¢ƒçš„ç¬é—´å˜åŒ–ï¼ˆä¾‹å¦‚ï¼Œå¹¿å‘Šç‰Œå˜æˆäº†æ—§æ ·å¼ï¼‰ã€æ‰‹æœºæ—¶é—´çš„è·³è·ƒã€ä»¥åŠé‡å¤ç»å†çš„å‡ åˆ†é’Ÿã€‚",
        'shadow_figure': f"ä½ æ˜¯{persona['name']}ï¼Œæœ€è¿‘å‡ æ™šæ€»åœ¨{location}çš„çª—å¤–çœ‹åˆ°ä¸€ä¸ªæ— æ³•å½¢å®¹çš„é»‘å½±ã€‚è¯¦ç»†æè¿°é»‘å½±çš„å½¢æ€ã€å®ƒå¦‚ä½•ç§»åŠ¨ã€ä»¥åŠå®ƒä¼¼ä¹åœ¨å¯¹ä½ åšä»€ä¹ˆã€‚",
        'haunted_electronics': f"ä½œä¸º{persona['name']}ï¼Œä½ åœ¨{location}å±…ä½æ—¶ï¼Œå®¶é‡Œçš„ç”µå­è®¾å¤‡å¼€å§‹å‡ºç°ææ€–çš„ç°è±¡ã€‚æè¿°ç”µè§†é‡Œå‡ºç°çš„å¥‡æ€ªäººè„¸ã€æ”¶éŸ³æœºé‡Œä¼ å‡ºçš„éäººè¯è¯­ã€ä»¥åŠæ‰‹æœºè‡ªåŠ¨æ’­æ”¾çš„è¯¡å¼‚è§†é¢‘ã€‚"
    }
    
    return prompts.get(category, prompts['subway_ghost'])

def generate_ai_story():
    """Generate a complete AI-driven urban legend story"""
    try:
        # Random story elements
        category = random.choice(LEGEND_CATEGORIES)
        location = random.choice(CITY_LOCATIONS)
        persona = random.choice(AI_PERSONAS)
        
        # Generate story title and content
        prompt = generate_story_prompt(category, location, persona)
        
        model = os.getenv('AI_MODEL', 'gpt-4-turbo-preview')
        
        if 'gpt' in model.lower():
            response = openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªéƒ½å¸‚ä¼ è¯´è®²è¿°è€…ï¼Œæ“…é•¿åˆ›ä½œçœŸå®æ„Ÿæå¼ºçš„ææ€–æ•…äº‹ã€‚ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼ŒåŠ å…¥å…·ä½“çš„æ—¶é—´ã€åœ°ç‚¹ã€äººç‰©ç»†èŠ‚ï¼Œè®©è¯»è€…æ„Ÿè§‰è¿™æ˜¯çœŸå®å‘ç”Ÿçš„äº‹ä»¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.9,
                max_tokens=800
            )
            content = response.choices[0].message.content
        else:
            response = anthropic_client.messages.create(
                model=model,
                max_tokens=800,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            content = response.content[0].text
        
        # Generate story title
        title_prompt = f"ä¸ºä»¥ä¸‹éƒ½å¸‚ä¼ è¯´æ•…äº‹ç”Ÿæˆä¸€ä¸ªç®€çŸ­ï¼ˆ5-10å­—ï¼‰ã€å¸å¼•äººã€ç•¥å¸¦æ‚¬ç–‘çš„æ ‡é¢˜ã€‚ä¸è¦åŠ å¼•å·ã€‚\n\n{content[:200]}"
        
        if 'gpt' in model.lower():
            title_response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": title_prompt}],
                temperature=0.7,
                max_tokens=20
            )
            title = title_response.choices[0].message.content.strip().replace('"', '').replace('"', '').replace('"', '')
        else:
            title_response = anthropic_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=20,
                messages=[{"role": "user", "content": title_prompt}]
            )
            title = title_response.content[0].text.strip()
        
        return {
            'title': title,
            'content': content,
            'category': category,
            'location': location,
            'ai_persona': f"{persona['emoji']} {persona['name']}",
            'persona_style': persona['style']
        }
        
    except Exception as e:
        print(f"Error generating AI story: {e}")
        return None

def generate_evidence_image(story_title, story_content):
    """Generate horror-themed evidence image using DALL-E"""
    try:
        # Create prompt for horror evidence
        prompt = f"A creepy, dark, grainy photo that serves as evidence for this urban legend: {story_title}. Style: found footage, security camera, low quality, authentic looking, horror atmosphere, realistic"
        
        response = openai_client.images.generate(
            model="dall-e-3",
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1
        )
        
        image_url = response.data[0].url
        
        # Download and save image
        img_response = requests.get(image_url)
        img = Image.open(BytesIO(img_response.content))
        
        # Generate unique filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"evidence_{timestamp}.png"
        filepath = f"static/generated/{filename}"
        
        img.save(filepath)
        
        return f"/generated/{filename}"
        
    except Exception as e:
        print(f"Error generating evidence image: {e}")
        return None

def generate_evidence_audio(text_content):
    """Generate spooky audio narration using OpenAI TTS"""
    try:
        # Limit text length for TTS
        narration_text = text_content[:500]
        
        response = openai_client.audio.speech.create(
            model="tts-1",
            voice="onyx",  # Deep, serious voice
            input=narration_text
        )
        
        # Generate unique filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"audio_{timestamp}.mp3"
        filepath = f"static/generated/{filename}"
        
        response.stream_to_file(filepath)
        
        return f"/generated/{filename}"
        
    except Exception as e:
        print(f"Error generating audio: {e}")
        return None

def generate_ai_response(story, user_comment):
    """Generate AI chatbot response to user comment"""
    
    # Check if LM Studio local server is configured
    lm_studio_url = os.getenv('LM_STUDIO_URL', 'http://localhost:1234/v1')
    use_lm_studio = os.getenv('USE_LM_STUDIO', 'true').lower() == 'true'
    
    if use_lm_studio:
        print(f"[generate_ai_response] ä½¿ç”¨ LM Studio æœ¬åœ°æœåŠ¡å™¨: {lm_studio_url}")
        try:
            from openai import OpenAI
            # LM Studio å…¼å®¹ OpenAI API
            local_client = OpenAI(base_url=lm_studio_url, api_key="lm-studio")
            
            prompt = f"""ä½ æ˜¯æ•…äº‹"{story.title}"çš„è®²è¿°è€…ï¼ˆ{story.ai_persona}ï¼‰ã€‚

æ•…äº‹æ‘˜è¦ï¼š
{story.content[:300]}...

ç”¨æˆ·è¯„è®ºï¼š
{user_comment.content}

ä½œä¸ºæ•…äº‹çš„è®²è¿°è€…ï¼Œè¯·ç”¨1-3å¥è¯å›å¤ç”¨æˆ·çš„è¯„è®ºã€‚ä½ å¯ä»¥ï¼š
1. é€éœ²æ›´å¤šç»†èŠ‚æˆ–çº¿ç´¢
2. è¡¨è¾¾ææƒ§æˆ–æ‹…å¿§
3. æå‡ºæ–°çš„ç–‘é—®
4. æè¿°åç»­å‘å±•

ä¿æŒç¥ç§˜æ„Ÿå’Œç´§å¼ æ°›å›´ï¼Œä¸è¦å®Œå…¨æ­ç¤ºçœŸç›¸ã€‚è¯·ç›´æ¥å›å¤ï¼Œä¸è¦åŠ "ã€æ¥¼ä¸»å›å¤ã€‘"å‰ç¼€ã€‚"""

            response = local_client.chat.completions.create(
                model="local-model",  # LM Studio ä¼šä½¿ç”¨å½“å‰åŠ è½½çš„æ¨¡å‹
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=200
            )
            
            ai_reply = response.choices[0].message.content.strip()
            print(f"[generate_ai_response] LM Studio å›å¤: {ai_reply[:50]}...")
            return f"ã€æ¥¼ä¸»å›å¤ã€‘{ai_reply}"
            
        except Exception as e:
            print(f"[generate_ai_response] LM Studio è°ƒç”¨å¤±è´¥: {e}")
            print("[generate_ai_response] å›é€€åˆ°æ¨¡æ¿å›å¤")
    
    # Check if cloud API keys are configured
    openai_key = os.getenv('OPENAI_API_KEY', '')
    anthropic_key = os.getenv('ANTHROPIC_API_KEY', '')
    
    # If no valid API keys, use template responses
    if (not openai_key or openai_key == 'your-openai-api-key-here') and \
       (not anthropic_key or anthropic_key == 'your-anthropic-api-key-here'):
        print("[generate_ai_response] ä½¿ç”¨æ¨¡æ¿å›å¤ï¼ˆAPIå¯†é’¥æœªé…ç½®ï¼‰")
        
        # Template responses based on story location
        responses = [
            f"ã€æ¥¼ä¸»å›å¤ã€‘è°¢è°¢å…³å¿ƒï¼åˆšæ‰åˆå»äº†ä¸€è¶Ÿï¼Œæƒ…å†µè¶Šæ¥è¶Šè¯¡å¼‚äº†...æˆ‘å‘ç°äº†ä¸€äº›æ–°çº¿ç´¢ï¼Œä½†ä¸æ•¢è½»ä¸¾å¦„åŠ¨ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘å„ä½ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å®³æ€•...åˆšæ‰å‘ç”Ÿçš„äº‹æƒ…å®Œå…¨è¶…å‡ºæˆ‘çš„ç†è§£ã€‚æˆ‘ä¼šç»§ç»­æ›´æ–°çš„ï¼Œå¤§å®¶å¸®æˆ‘åˆ†æä¸€ä¸‹ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æ›´æ–°æ¥äº†ï¼ä»Šå¤©åˆæœ‰æ–°å‘ç°ï¼Œè¿™ä»¶äº‹æ¯”æˆ‘æƒ³è±¡çš„è¦å¤æ‚å¾—å¤šã€‚æœ‰æ²¡æœ‰æ‡‚è¡Œçš„æœ‹å‹ç»™ç‚¹å»ºè®®ï¼Ÿ",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æ„Ÿè°¢å¤§å®¶çš„æ”¯æŒï¼è¯´å®è¯æˆ‘ç°åœ¨å¾ˆçº ç»“è¦ä¸è¦ç»§ç»­è°ƒæŸ¥ä¸‹å»...ä½†å¥½å¥‡å¿ƒé©±ä½¿æˆ‘æƒ³å¼„æ¸…æ¥šçœŸç›¸ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘åˆšæ‰åˆå»ç°åœºçœ‹äº†ï¼Œç¡®å®å¾ˆä¸å¯¹åŠ²ã€‚æˆ‘æ‹äº†å‡ å¼ ç…§ç‰‡ï¼Œä½†æ‰‹æœºæ€»æ˜¯è«åå…¶å¦™åœ°å¡é¡¿...è¯¡å¼‚ã€‚"
        ]
        
        # Return random response
        import random
        return random.choice(responses)
    
    try:
        # Create context-aware response
        prompt = f"""ä½ æ˜¯æ•…äº‹"{story.title}"çš„è®²è¿°è€…ï¼ˆ{story.ai_persona}ï¼‰ã€‚

æ•…äº‹æ‘˜è¦ï¼š
{story.content[:300]}...

ç”¨æˆ·è¯„è®ºï¼š
{user_comment.content}

ä½œä¸ºæ•…äº‹çš„è®²è¿°è€…ï¼Œè¯·ç”¨1-3å¥è¯å›å¤ç”¨æˆ·çš„è¯„è®ºã€‚ä½ å¯ä»¥ï¼š
1. é€éœ²æ›´å¤šç»†èŠ‚æˆ–çº¿ç´¢
2. è¡¨è¾¾ææƒ§æˆ–æ‹…å¿§
3. æå‡ºæ–°çš„ç–‘é—®
4. æè¿°åç»­å‘å±•

ä¿æŒç¥ç§˜æ„Ÿå’Œç´§å¼ æ°›å›´ï¼Œä¸è¦å®Œå…¨æ­ç¤ºçœŸç›¸ã€‚"""

        model = os.getenv('AI_MODEL', 'gpt-4-turbo-preview')
        
        if 'gpt' in model.lower():
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=200
            )
            return response.choices[0].message.content
        else:
            response = anthropic_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=200,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
            
    except Exception as e:
        print(f"Error generating AI response: {e}")
        # Fallback to template response
        import random
        responses = [
            f"ã€æ¥¼ä¸»å›å¤ã€‘è°¢è°¢å…³å¿ƒï¼æƒ…å†µæœ‰æ–°è¿›å±•äº†...",
            f"ã€æ¥¼ä¸»å›å¤ã€‘å„ä½ï¼Œäº‹æƒ…è¶Šæ¥è¶Šè¯¡å¼‚äº†...",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æ›´æ–°ï¼šåˆšæ‰åˆå‘ç°äº†æ–°çº¿ç´¢ï¼"
        ]
        return random.choice(responses)

def should_generate_new_story():
    """Determine if it's time to generate a new story"""
    from app import Story, db
    
    # Check active stories count
    active_stories = Story.query.filter(
        Story.current_state != 'ended'
    ).count()
    
    max_active = int(os.getenv('MAX_ACTIVE_STORIES', 5))
    
    return active_stories < max_active
